---
title: ETC4500/ETC5450 Advanced&nbsp;R&nbsp;programming
author: "Week 5: Reactive programming with targets and renv"
format:
  presentation-beamer:
    fontsize: "14pt,t"
    section-titles: false
    knitr:
      opts_chunk:
        dev: "cairo_pdf"
    fig-width: 7.5
    fig-height: 3.5
    include-in-header: ../header.tex
    colorlinks: true
    urlcolor: MonashBlue
    linkcolor: burntorange
---

```{r}
#| label: setup
#| include: false
#| cache: false
source(here::here("setup.R"))
source(here::here("course_info.R"))
```

# targets

## targets: reproducible computation at scale

\placefig{0.5}{1.8}{width=5cm}{images/logo.png}

\begin{textblock}{15}(0.5,8.5)
\textcolor{gray}{\footnotesize Some images from https://wlandau.github.io/targets-tutorial}
\end{textblock}

\begin{textblock}{10}(6, 2)
\begin{itemize}
\item Supports a clean, modular, function-oriented programming style.
\item Learns how your pipeline fits together.
\item Runs only the necessary computation.
\item Abstracts files as R objects.
\item Similar to Makefiles, but with R functions.
\end{itemize}
\end{textblock}

## Interconnected tasks

\only<1>{\placefig{0.5}{2}{width=13cm}{images/workflow.png}}
\only<2>{\placefig{0.5}{2}{width=13cm}{images/change.png}}
\only<3>{\placefig{0.5}{2}{width=13cm}{images/downstream.png}}

## Dilemma: short runtimes or reproducible results?

\fullheight{images/decisions.png}

## Let a pipeline tool do the work

\fullwidth{images/pipeline_graph.png}\vspace*{-0.15cm}

* Save time while ensuring computational reproducibility.
* Automatically skip tasks that are already up to date.

## Typical project structure

```{verbatim}
_targets.R # Required top-level configuration file.
R/
└── functions.R
data/
└── my_data.csv
```

### _targets.R
\vspace*{-0.26cm}

```{r}
#| eval: false
library(targets)
tar_source() # source all files in R folder
tar_option_set(packages = c("tidyverse", "fable"))
list(
  tar_target(my_file, "data/my_data.csv", format = "file"),
  tar_target(my_data, read_csv(my_file)),
  tar_target(my_model, model_function(my_data))
)
```

## Generate `_targets.R` in working directory

```{r}
#| eval: false
library(targets)
tar_script()
```


## Useful targets commands

* `tar_make()` to run the pipeline.
* `tar_make(starts_with("fig"))` to run only targets starting with "fig".
* `tar_read(object)` to read a target.
* `tar_load(object)` to load a target.
* `tar_load_everything()` to load all targets.
* `tar_manifest()` to list all targets
* `tar_visnetwork()` to visualize the pipeline.
* `tar_destroy()` to remove all targets.
* `tar_outdated()` to list outdated targets.

## Debugging

Errored targets to return `NULL` so pipeline continues.

```{r}
#| eval: false
tar_option_set(error = "null")
```

\pause

See error messages for all targets.

```{r}
#| eval: false
tar_meta(fields = error, complete_only = TRUE)
```

\pause

See warning messages for all targets.

```{r}
#| eval: false
tar_meta(fields = warnings, complete_only = TRUE)
```

## Debugging
\fontsize{14}{15.5}\sf

* Try loading all available targets: `tar_load_everything()`. Then run the command of the errored target in the console.

* Pause the pipeline with `browser()`

* Use the debug option: `tar_option_set(debug = "target_name")`

* Save the workspaces:

  - `tar_option_set(workspace_on_error = TRUE)`
  - `tar_workspaces()`
  - `tar_workspace(target_name)`


## Random numbers

* Each target runs with its own seed based on its name and the global seed from `tar_option_set(seed = ???)`
* So running only some targets, or running them in a different order, will not change the results.

## Folder structure

```{verbatim}
├── .git/
├── .Rprofile
├── .Renviron
├── renv/
├── index.Rmd
├── _targets/
├── _targets.R
├── _targets.yaml
├── R/
├──── functions_data.R
├──── functions_analysis.R
├──── functions_visualization.R
├── data/
└──── input_data.csv
```

## targets with quarto

```{r}
#| eval: false
library(targets)
library(tarchetypes)                                                 # <1>
tar_source() # source all files in R folder
tar_option_set(packages = c("tidyverse", "fable"))
list(
  tar_target(my_file, "data/my_data.csv", format = "file"),
  tar_target(my_data, read_csv(my_file)),
  tar_target(my_model, model_function(my_data))
  tar_quarto(report, "file.qmd", extra_files = "references.bib")     # <2>
  )
```

1. Load `tarchetypes` package for quarto support.
2. Add a quarto target.

## Exercise

* Add a targets workflow to your quarto document.
* Create a visualization of the pipeline network using `tar_visnetwork()`.

# Reproducible environments

## Reproducible environments

* To ensure that your code runs the same way on different machines and at different times, you need the computing environment to be the same.
  1. Operating system
  2. System components
  3. R version
  4. R packages

* Solutions for 1--4: Docker, Singularity, `containerit`, `rang`
* Solutions for 4: `packrat`, `checkpoint`, `renv`

## renv package

![](../diagrams/renv.png)

## renv package

* `renv::init()` : initialize a new project with a new environment. Adds:
  *  `renv/library` contains all packages used in project
  *  `renv.lock` contains metadata about packages used in project
  *  `.Rprofile` run every time R starts.

* `renv::snapshot()` : save the state of the project to `renv.lock`.

* `renv::restore()` : restore the project to the state saved in `renv.lock`.

## renv package
\fontsize{14}{16}\sf

* renv uses a package cache so you are not repeatedly installing the same packages in multiple projects.
* `renv::install()` can install from CRAN, Bioconductor, GitHub, Gitlab, Bitbucket, etc.
* `renv::update()` gets latest versions of all dependencies from wherever they were installed from.
* Only R packages are supported, not system dependencies, and not R itself.
* renv is not a replacement for Docker or Singularity.
* `renv::deactivate(clean = TRUE)` will remove the renv environment.
